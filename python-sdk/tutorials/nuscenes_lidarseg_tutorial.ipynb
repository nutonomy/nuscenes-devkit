{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nuScenes-lidarseg tutorial\n",
    "\n",
    "Welcome to the nuScenes-lidarseg tutorial.\n",
    "\n",
    "This demo assumes that nuScenes is installed at `/data/sets/nuscenes`. The mini version (i.e. v1.0-mini) of the full dataset will be used for this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To install the nuScenes-lidarseg expansion, download the dataset from https://www.nuscenes.org/download. Unpack the compressed file(s) into `/data/sets/nuscenes` and your folder structure should end up looking like this:\n",
    "```\n",
    "└── nuscenes  \n",
    "    ├── Usual nuscenes folders (i.e. samples, sweep)\n",
    "    │\n",
    "    ├── lidarseg\n",
    "    │   └── v1.0-{mini, trainval} <- Contains the .bin files; a .bin file \n",
    "    │                                contains the labels of the points in a \n",
    "    │                                point cloud (note that v1.0-test does not \n",
    "    │                                have any .bin files associated with it)                          \n",
    "    └── v1.0-{mini, trainval, test}\n",
    "        ├── Usual files (e.g. attribute.json, calibrated_sensor.json etc.)     \n",
    "        ├── lidarseg.json  <- contains the mapping of each .bin file to the token  \n",
    "        └── lidarseg_category.json <- contains the categories of the labels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "33 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.4 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.expanduser('~/Desktop/nuscenes-devkit/python-sdk'))\n",
    "from nuscenes import NuScenes\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you do not need any extra libraries to use nuScenes-lidarseg. The original nuScenes devkit which you are familiar with has been extended so that you can use it seamlessly with nuScenes-lidarseg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics of lidarseg dataset for the v1.0-mini split\n",
    "Let's get a quick feel of the lidarseg dataset by looking at what classes are in it and the number of points belonging to each class. The classes will be sorted in ascending order based on the number of points (since `sort_counts=True` below; if you wish to sort the statistics by alphabetical order, just set `sort_counts=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stats for nuScenes-lidarseg...\n",
      "  3  human.pedestrian.wheelchair              nbr_points=           0\n",
      "  4  human.pedestrian.stroller                nbr_points=           0\n",
      "  8  animal                                   nbr_points=           0\n",
      " 16  vehicle.emergency.ambulance              nbr_points=           0\n",
      " 17  vehicle.emergency.police                 nbr_points=           0\n",
      " 31  vehicle.ego                              nbr_points=           0\n",
      " 32  vehicle.emergency.firetruck              nbr_points=           0\n",
      " 22  movable_object.debris                    nbr_points=          48\n",
      "  6  human.pedestrian.police_officer          nbr_points=          64\n",
      "  2  human.pedestrian.child                   nbr_points=         230\n",
      "  7  human.pedestrian.construction_worker     nbr_points=       1,412\n",
      " 11  vehicle.bicycle                          nbr_points=       1,463\n",
      " 21  movable_object.pushable_pullable         nbr_points=       2,293\n",
      "  5  human.pedestrian.personal_mobility       nbr_points=       4,096\n",
      " 23  static_object.bicycle_rack               nbr_points=       4,476\n",
      " 20  movable_object.trafficcone               nbr_points=       6,206\n",
      " 10  vehicle.motorcycle                       nbr_points=       6,713\n",
      "  0  noise                                    nbr_points=      12,561\n",
      " 18  vehicle.trailer                          nbr_points=      12,787\n",
      " 13  vehicle.bus.rigid                        nbr_points=      29,694\n",
      " 15  vehicle.construction                     nbr_points=      39,300\n",
      " 12  vehicle.bus.bendy                        nbr_points=      40,536\n",
      "  1  human.pedestrian.adult                   nbr_points=      43,812\n",
      " 19  movable_object.barrier                   nbr_points=      55,298\n",
      " 27  flat.other                               nbr_points=     150,153\n",
      " 14  vehicle.truck                            nbr_points=     304,234\n",
      "  9  vehicle.car                              nbr_points=     521,237\n",
      " 26  flat.terrain                             nbr_points=     696,526\n",
      " 25  flat.sidewalk                            nbr_points=     746,980\n",
      " 29  static.vegetation                        nbr_points=   1,565,272\n",
      " 28  static.manmade                           nbr_points=   2,067,510\n",
      " 30  static.other                             nbr_points=   3,643,428\n",
      " 24  flat.driveable_surface                   nbr_points=   4,069,879\n",
      "Calculated stats for 404 point clouds in 0.1 seconds.\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "nusc.list_lidarseg_categories(sort_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `list_lidarseg_categories`, you can get the index which each class name belongs to by looking at the leftmost column. You can also get a mapping of the indices to the class names from the `lidarseg_idx2name_mapping` attribute of the NuScenes class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'noise',\n",
       " 1: 'human.pedestrian.adult',\n",
       " 2: 'human.pedestrian.child',\n",
       " 3: 'human.pedestrian.wheelchair',\n",
       " 4: 'human.pedestrian.stroller',\n",
       " 5: 'human.pedestrian.personal_mobility',\n",
       " 6: 'human.pedestrian.police_officer',\n",
       " 7: 'human.pedestrian.construction_worker',\n",
       " 8: 'animal',\n",
       " 9: 'vehicle.car',\n",
       " 10: 'vehicle.motorcycle',\n",
       " 11: 'vehicle.bicycle',\n",
       " 12: 'vehicle.bus.bendy',\n",
       " 13: 'vehicle.bus.rigid',\n",
       " 14: 'vehicle.truck',\n",
       " 15: 'vehicle.construction',\n",
       " 16: 'vehicle.emergency.ambulance',\n",
       " 17: 'vehicle.emergency.police',\n",
       " 18: 'vehicle.trailer',\n",
       " 19: 'movable_object.barrier',\n",
       " 20: 'movable_object.trafficcone',\n",
       " 21: 'movable_object.pushable_pullable',\n",
       " 22: 'movable_object.debris',\n",
       " 23: 'static_object.bicycle_rack',\n",
       " 24: 'flat.driveable_surface',\n",
       " 25: 'flat.sidewalk',\n",
       " 26: 'flat.terrain',\n",
       " 27: 'flat.other',\n",
       " 28: 'static.manmade',\n",
       " 29: 'static.vegetation',\n",
       " 30: 'static.other',\n",
       " 31: 'vehicle.ego',\n",
       " 32: 'vehicle.emergency.firetruck'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nusc.lidarseg_idx2name_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, you can get the mapping of the class names to the indices from the `lidarseg_name2idx_mapping` attribute of the NuScenes class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noise': 0,\n",
       " 'human.pedestrian.adult': 1,\n",
       " 'human.pedestrian.child': 2,\n",
       " 'human.pedestrian.wheelchair': 3,\n",
       " 'human.pedestrian.stroller': 4,\n",
       " 'human.pedestrian.personal_mobility': 5,\n",
       " 'human.pedestrian.police_officer': 6,\n",
       " 'human.pedestrian.construction_worker': 7,\n",
       " 'animal': 8,\n",
       " 'vehicle.car': 9,\n",
       " 'vehicle.motorcycle': 10,\n",
       " 'vehicle.bicycle': 11,\n",
       " 'vehicle.bus.bendy': 12,\n",
       " 'vehicle.bus.rigid': 13,\n",
       " 'vehicle.truck': 14,\n",
       " 'vehicle.construction': 15,\n",
       " 'vehicle.emergency.ambulance': 16,\n",
       " 'vehicle.emergency.police': 17,\n",
       " 'vehicle.trailer': 18,\n",
       " 'movable_object.barrier': 19,\n",
       " 'movable_object.trafficcone': 20,\n",
       " 'movable_object.pushable_pullable': 21,\n",
       " 'movable_object.debris': 22,\n",
       " 'static_object.bicycle_rack': 23,\n",
       " 'flat.driveable_surface': 24,\n",
       " 'flat.sidewalk': 25,\n",
       " 'flat.terrain': 26,\n",
       " 'flat.other': 27,\n",
       " 'static.manmade': 28,\n",
       " 'static.vegetation': 29,\n",
       " 'static.other': 30,\n",
       " 'vehicle.ego': 31,\n",
       " 'vehicle.emergency.firetruck': 32}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nusc.lidarseg_name2idx_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a sample token\n",
    "Let's pick a sample to use for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = nusc.sample[87]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get statistics of a lidarseg sample token\n",
    "Now let's take a look at what classes are present in the pointcloud of this particular sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.get_sample_lidarseg_stats(my_sample['token'], sort_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing ```sort_counts=True```, the classes and their respective frequency counts are printed in ascending order; On the other hand, ```sort_counts=False``` will print the classes and their respective frequency counts in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render the lidarseg labels in the bird's eye view of a pointcloud\n",
    "In the original nuScenes devkit, you would pass a sample data token into ```render_sample_data``` to render a bird's eye view of the pointcloud. However, the points would be colored according to the distance from the ego vehicle. Now with the extended nuScenes devkit, all you need to do is set ```show_lidarseg=True``` to visualize the class labels of the pointcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_token = my_sample['data']['LIDAR_TOP']\n",
    "nusc.render_sample_data(sample_data_token,\n",
    "                        with_anns=False,\n",
    "                        show_lidarseg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you wanted to focus on only certain classes? Given the statistics of the pointcloud printed out previously, let's say you are only interested in trucks and trailers. You could see the class indices belonging to those classes from the statistics and then pass an array of those indices into ```filter_lidarseg_labels``` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_sample_data(sample_data_token,\n",
    "                        with_anns=False,\n",
    "                        show_lidarseg=True,\n",
    "                        filter_lidarseg_labels=[9, 14, 18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only points in the pointcloud belonging to trucks and trailers are filtered out for your viewing pleasure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render lidarseg labels in image\n",
    "If you wanted to superimpose the pointcloud into the corresponding image from a camera, you can use ```render_pointcloud_in_image``` like what you would do with the original nuScenes devkit, but set ```show_lidarseg=True``` (remember to set ```render_intensity=False```). Similar to ```render_sample_data```, you can filter to see only certain classes using ```filter_lidarseg_labels```. And you can use ```show_lidarseg_legend``` to display a legend in the rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_pointcloud_in_image(my_sample['token'],\n",
    "                                pointsensor_channel='LIDAR_TOP',\n",
    "                                camera_channel='CAM_BACK',\n",
    "                                render_intensity=False,\n",
    "                                show_lidarseg=True,\n",
    "                                filter_lidarseg_labels=[9, 14, 18],\n",
    "                                show_lidarseg_legend=True,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render sample (i.e. lidar, radar and all cameras)\n",
    "Of course, like in the original nuScenes devkit, you can render all the sensors at once with ```render_sample```. In this extended nuScenes devkit, you can set ```show_lidarseg=True``` to see the lidarseg labels. Similar to the above methods, you can use ```filter_lidarseg_labels``` to display only the classes you wish to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_sample(my_sample['token'],\n",
    "                   show_lidarseg=True,\n",
    "                   filter_lidarseg_labels=[9, 14, 18],\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render a scene for a given camera sensor with lidarseg labels\n",
    "You can also render an entire scene with the lidarseg labels for a camera of your choosing (the ```filter_lidarseg_labels``` argument can be used here as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a scene first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene = nusc.scene[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the scene token into ```render_scene_channel_lidarseg``` indicating that we are only interested in construction vehicles and man-made objects.\n",
    "\n",
    "(Note: the following code is commented out as it crashes in Jupyter notebooks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# nusc.render_scene_channel_lidarseg(my_scene['token'], \n",
    "#                                    'CAM_BACK', \n",
    "#                                    filter_lidarseg_labels=[15, 30],\n",
    "#                                    verbose=True, \n",
    "#                                    imsize=(1280, 720))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the renderings, you can pass a path to a folder you want to save the images to via the ```out_folder``` argument, and either `video` or `image` to `render_mode`.\n",
    "\n",
    "(Note: the following code is commented out as it crashes in Jupyter notebooks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nusc.render_scene_channel_lidarseg(my_scene['token'],\n",
    "#                                    'CAM_BACK',\n",
    "#                                    filter_lidarseg_labels=[15, 30],\n",
    "#                                    verbose=True,\n",
    "#                                    imsize=(1280, 720),\n",
    "#                                    render_mode='video',\n",
    "#                                    out_folder=os.path.expanduser('~/Desktop/my_folder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `render_mode='image'`, only frames which contain points (after the filter has been applied) will be saved as images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render a scene for all cameras with lidarseg labels\n",
    "You can also render the entire scene for all cameras at once with the lidarseg labels as a video. Let's say in this case, we are interested in points belonging to driveable surfaces and cars.\n",
    "\n",
    "(Note: the following code is commented out as it crashes in Jupyter notebooks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nusc.render_scene_lidarseg(my_scene['token'], \n",
    "#                            filter_lidarseg_labels=[26, 9],\n",
    "#                            dpi=100,\n",
    "#                            out_path=os.path.expanduser('~/Desktop/my_rendered_scene.avi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing LIDAR segmentation predictions\n",
    "In all the above functions, the labels of the LIDAR pointcloud which have been rendered are the ground truth. If you have trained a model to segment LIDAR pointclouds and have run it on the nuScenes-lidarseg dataset, you can visualize your model's predictions with nuScenes-lidarseg as well!\n",
    "\n",
    "You simply need to pass the path to the .bin file where your predictions for the given sample are to `lidarseg_preds_bin_path` for these functions:\n",
    "- `list_lidarseg_categories`\n",
    "- `render_sample_data`\n",
    "- `render_pointcloud_in_image`\n",
    "- `render_sample`                 \n",
    "\n",
    "For example, let's assume the predictions for `my_sample` is stored at `/data/sets/nuscenes/lidarseg/v1.0-mini` with the format `<lidar_sample_data_token>_lidarseg.bin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "my_sample = nusc.sample[87]\n",
    "sample_data_token = my_sample['data']['LIDAR_TOP']\n",
    "my_predictions_bin_file = os.path.join('/data/sets/nuscenes/lidarseg/v1.0-mini', sample_data_token + '_lidarseg.bin')\n",
    "\n",
    "nusc.render_pointcloud_in_image(my_sample['token'],\n",
    "                                pointsensor_channel='LIDAR_TOP',\n",
    "                                camera_channel='CAM_BACK',\n",
    "                                render_intensity=False,\n",
    "                                show_lidarseg=True,\n",
    "                                filter_lidarseg_labels=[9, 14, 18],\n",
    "                                show_lidarseg_legend=True,\n",
    "                                verbose=True,\n",
    "                                lidarseg_preds_bin_path=my_predictions_bin_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these functions that render an entire scene, you will need to pass the path to the folder which contains the .bin files for each sample in a scene to `lidarseg_preds_folder`:\n",
    "- `render_scene_channel_lidarseg`\n",
    "- `render_scene_lidarseg`\n",
    "\n",
    "Pay special attention that **each set of predictions in the folder _must_ be a `.bin` file and named as `<lidar_sample_data_token>_lidarseg.bin`**.\n",
    "\n",
    "(Note: the following code is commented out as it crashes in Jupyter notebooks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_scene = nusc.scene[0]\n",
    "# my_folder_of_predictions = '/data/sets/nuscenes/lidarseg/v1.0-mini'\n",
    "\n",
    "# nusc.render_scene_channel_lidarseg(my_scene['token'], \n",
    "#                                    'CAM_BACK', \n",
    "#                                    filter_lidarseg_labels=[15, 30],\n",
    "#                                    verbose=True, \n",
    "#                                    imsize=(1280, 720),\n",
    "#                                    lidarseg_preds_folder=my_folder_of_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "And this brings us to the end of the tutorial for nuScenes-lidarseg, enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
